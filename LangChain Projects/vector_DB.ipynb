{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AaFgtlxbwCL"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain openai tiktoken langchain-community"
      ],
      "metadata": {
        "id": "XuAgNRRwcaQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY']=\"Place_your_api_key_here\""
      ],
      "metadata": {
        "id": "wtxEyxencbSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import some libraries"
      ],
      "metadata": {
        "id": "bkV9K7dFc12K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from lanchain.document_loaders import DirectoryLoader"
      ],
      "metadata": {
        "id": "fUu0duvEcbO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=DirectoryLoader('./your_folder_path',glob=\"*.txt\",loader_cls=TextLoader)\n",
        "document=loader.load()"
      ],
      "metadata": {
        "id": "z73BBtIQcbLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this will load the entire data inside the folder. So we need to devide it in different chunks. cause every llm model has some fixed token length which they can take input as."
      ],
      "metadata": {
        "id": "86Eap0DZhHhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "texts=text_splitter.split_documents(document) # this will return an array of string with 1000 characters each"
      ],
      "metadata": {
        "id": "W--kOjEihG-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating vector DB"
      ],
      "metadata": {
        "id": "Qvk-SV50h4Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lanchain import embeddings\n",
        "persist_directory='db'\n",
        "embeddings=openAIEmbeddings()\n",
        "vectordb=Chroma.from_documents(documents=texts,embedding=embeddings,persist_directory=persist_directory)\n"
      ],
      "metadata": {
        "id": "sp60modDcbIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vector have been created now we need to save it in disk"
      ],
      "metadata": {
        "id": "vbNj4GrbiNbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# persist the db to disk\n",
        "vectordb.persist()\n",
        "vectordb=None"
      ],
      "metadata": {
        "id": "ikODbRkscbDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can load the persisted db from disk, and use it normally\n",
        "vectordb=Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "l5CE4Fopca86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Retriever"
      ],
      "metadata": {
        "id": "-fbnHFN_ik70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vectordb.as_retriever()\n",
        "retriever=vectordb.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":2}) # this will help us to get how many response we want"
      ],
      "metadata": {
        "id": "kqcfHw4ica2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs=retriever.get_relevant_documents(\"what is the meaning of life\") # this code will return chunks of data from the documents based on the similarity search\n",
        "# now we need to take help of llms to generate a valid response from the chunk of data and query we have provided."
      ],
      "metadata": {
        "id": "VHkme-TVcaur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create chain"
      ],
      "metadata": {
        "id": "qAyfOD7io6RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "j_fmGbfbcajm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain=RetrievalQA.from_chain_type(llm=OpenAI(),chain_type=\"stuff\",retriever=retriever,return_source_documents=True)\n"
      ],
      "metadata": {
        "id": "e6HpbP_e73-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"what is the meaning of life\"\n",
        "result=qa_chain({\"query\":query})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "ubNoJczP79Sd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}